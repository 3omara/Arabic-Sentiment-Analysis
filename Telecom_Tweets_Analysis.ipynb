{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\youss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataCleaner import preProcess\n",
    "from helper_fns import lexicon_calculate\n",
    "from Sequencer import Sequencer\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klbe1997</td>\n",
       "      <td>الوادي الجديد</td>\n",
       "      <td>شعري لونه orange وانتوا؟!</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SaMar2555_</td>\n",
       "      <td>الوادي الجديد</td>\n",
       "      <td>متعتك عندي بتحويل فودافون كاش سكس فون و كاميرا...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dyna86889772</td>\n",
       "      <td>الوادي الجديد</td>\n",
       "      <td>شعري لونه orange وانتوا؟!</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hcintma</td>\n",
       "      <td>الوادي الجديد</td>\n",
       "      <td>شعري لونه orange وانتوا؟!</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alikreek564</td>\n",
       "      <td>الوادي الجديد</td>\n",
       "      <td>شعري لونه orange وانتوا؟!</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_name    user_location  \\\n",
       "0      klbe1997   الوادي الجديد    \n",
       "1    SaMar2555_   الوادي الجديد    \n",
       "2  dyna86889772   الوادي الجديد    \n",
       "3       hcintma   الوادي الجديد    \n",
       "4   alikreek564   الوادي الجديد    \n",
       "\n",
       "                                                text  likes        date  \n",
       "0                          شعري لونه orange وانتوا؟!      0  2022-08-28  \n",
       "1  متعتك عندي بتحويل فودافون كاش سكس فون و كاميرا...      2  2022-08-28  \n",
       "2                          شعري لونه orange وانتوا؟!      0  2022-08-28  \n",
       "3                          شعري لونه orange وانتوا؟!      0  2022-08-28  \n",
       "4                          شعري لونه orange وانتوا؟!      1  2022-08-28  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('Datasets/tweets_sns2.csv')\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6876, 5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_regex = \"[Vv]odafone|VODAFONE|فودافون|[Ee]tisalat|ETISALAT|اتصالات|[Oo]range|ORANGE|اورانج|موبينيل|إتصالات|أورانج\"\n",
    "remove_regex = \"لون\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indeces = tweets_df[(tweets_df['text'].str.contains(keep_regex)==False) | (tweets_df['text'].str.contains(remove_regex)==True)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6348, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.drop(drop_indeces, inplace=True)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اعلانات اتصالات وحشاني 😂😂'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.iloc[179, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_indeces = tweets_df[tweets_df.duplicated('text')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2817, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.drop(duplicate_indeces, inplace=True)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('Datasets/tweets_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.rename(columns={\"text\":\"tweet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataCleaner import clean_arabic_text, farasa_stemmer, normalize, qalsadi_lemmatizer, remove_diacritics, remove_links, remove_longation, remove_tag_persons, replace_emojies, snow_ball_stemmer, stem_docs\n",
    "\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : remove_tag_persons(tweet))\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : remove_links(tweet))\n",
    "# tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : remove_diacritics(tweet))\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : replace_emojies(tweet))\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : clean_arabic_text(tweet))\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : normalize(tweet))\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : remove_longation(tweet))\n",
    "# tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : stem_docs(tweet))\n",
    "# tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : farasa_stemmer(tweet))\n",
    "# tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : qalsadi_lemmatizer(tweet))\n",
    "tweets_df['tweet'] = tweets_df.tweet.map(lambda tweet : snow_ball_stemmer(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(\"Preprocessing Trials/train_rtp_rl_re_cat_n_rl_snowball.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets= tweets_df['tweet'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Temp\\ipykernel_21020\\1748729805.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df2.append(df1,ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('Datasets/tweets_train.csv')\n",
    "df1 = pd.read_csv('Datasets/NU_EG_Twitter_corpus_train.csv')\n",
    "df2.rename(columns={\"text\":\"tweet\"},inplace=True)\n",
    "df = df2.append(df1,ignore_index = True)\n",
    "preProcess(df)\n",
    "tweets2 = df['tweet'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = set(nltk.tokenize.wordpunct_tokenize(' '.join(tweets.to_numpy().flatten())))\n",
    "tokens2 = set(nltk.tokenize.wordpunct_tokenize(' '.join(tweets2.to_numpy().flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7686 25581\n"
     ]
    }
   ],
   "source": [
    "print(len(set(tokens1)), len(set(tokens2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3202\n"
     ]
    }
   ],
   "source": [
    "print(len(set(tokens1).difference(set(tokens2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_filter(tweet: str, bucket: set[str]) -> bool:\n",
    "    tokens = set(nltk.tokenize.wordpunct_tokenize(tweet))\n",
    "    inclusion_ratio =  len(tokens.intersection(bucket)) / len(tokens)\n",
    "    return inclusion_ratio >= 0.6\n",
    "\n",
    "\n",
    "filtered_tweets = tweets.to_numpy()[[tweet_filter(x, tokens2) for x in tweets.to_numpy()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2637\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Models/nb_lexicon_model.sav', mode='rb') as f:\n",
    "    nb_lex_count_model = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
